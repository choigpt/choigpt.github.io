---
layout: post
title: Regularization
date: 2026-02-24 16:14:00
tags: [deep-learning, regularization, machine-learning]
category: Deep Learning
---

딥러닝 모델이 **과적합(Overfitting)을 피하도록** 제어하는 기법들의 총칭이다.

---

## 과적합(Overfitting)이란?

학습 데이터에만 지나치게 맞춰져, **새로운 데이터에 대한 예측 성능이 떨어지는 현상**이다.

### 편향-분산 트레이드오프

과적합을 이해하려면 편향(Bias)과 분산(Variance) 개념이 필요하다.

```
전체 오차 = Bias² + Variance + 노이즈(불가피)
```

- **Bias(편향)**: 모델이 근본적으로 얼마나 단순한가. 너무 단순하면 underfitting.
- **Variance(분산)**: 학습 데이터에 따라 모델이 얼마나 흔들리는가. 너무 높으면 overfitting.

복잡한 모델일수록 Bias는 낮고 Variance는 높다. Regularization은 **모델 복잡도를 제한해 Variance를 낮추는 대신 Bias를 약간 허용**하는 트레이드오프다.

Regularization은 이 최적 복잡도 지점을 향해 모델을 유도하는 역할을 한다.

---

## Regularization의 핵심 아이디어

손실 함수에 **가중치의 크기에 비례한 페널티를 추가**해서 모델이 가중치를 크게 키우는 걸 억제한다.

```
새로운 손실 L = 원래 손실(MSE) + λ · Penalty(w)
```

페널티를 더하면 손실이 커질 것 같지만, 실제로는 불필요한 가중치가 줄어들어 **일반화 성능이 높아진다**. λ는 정규화 강도를 조절하는 하이퍼파라미터다.

---

## L1 Regularization (Lasso)

가중치의 **절대값의 합**을 페널티로 추가한다.

```
L₁ = (ŷ - y)² + λ|w|
   = (w·x + b - y)² + λ|w|

∂L₁/∂w = 2x(w·x + b - y) + λ · d|w|/dw

w* = w - α · ∂L₁/∂w
```

`|w|`를 미분하면 **부호 함수(sign)** 가 나온다.

```
d|w|/dw = +λ  (w ≥ 0)
d|w|/dw = -λ  (w < 0)
```

따라서 업데이트는 다음과 같다.

```
w* = w - α · (A + λ)    (w ≥ 0)
w* = w - α · (A - λ)    (w < 0)
```

이 **일정한 크기의 압력**이 핵심이다. 가중치가 크든 작든 동일한 λ만큼 깎인다. 그러면 처음부터 작았던 가중치는 금방 0에 도달해버린다.

### 기하학적 해석 — 왜 L1은 0을 만드는가

최적해는 **MSE 등고선과 L1 제약 영역이 처음 만나는 점**이다. L1 제약 영역은 **마름모(다이아몬드)** 모양이다. 마름모의 꼭짓점은 어떤 축과 정렬되어 있어, 등고선이 꼭짓점에서 먼저 닿는 경우가 많다. 꼭짓점은 곧 특정 가중치가 **정확히 0인 지점**이다.

---

## L2 Regularization (Ridge)

가중치의 **제곱의 합**을 페널티로 추가한다.

```
L₂ = (ŷ - y)² + λw²
   = (w·x + b - y)² + λw²

∂L₂/∂w = 2x(w·x + b - y) + 2λw

w* = w - α · (A + 2λw)
   = w(1 - 2αλ) - α·A
```

미분값이 `2λw`로, **가중치 크기에 비례한 페널티**가 붙는다.

### 기하학적 해석 — 왜 L2는 0이 되지 않는가

L2 제약 영역은 **원(sphere)** 모양이다. 원의 경계에는 꼭짓점이 없다. 등고선이 만나는 점은 **원 위의 임의의 점**이 되며, 어떤 가중치도 정확히 0이 될 이유가 없다. 대신 **모든 가중치가 골고루 작아지는** 효과가 생긴다.

---

## λ 튜닝

λ는 정규화 강도를 결정하는 핵심 하이퍼파라미터다.

```
λ = 0         → 정규화 없음, 과적합 위험
λ 너무 큼    → 모든 가중치 → 0, 모델이 아무것도 못 배움 (underfitting)
λ 최적값    → 일반화 성능 최대
```

실전에서는 **Cross-Validation**으로 최적 λ를 찾는다. 일반적인 탐색 범위는 `{0.001, 0.01, 0.1, 1, 10}` 정도의 로그 스케일이다.

---

## Elastic Net

L1과 L2를 **혼합**한 방식이다.

```
L_elastic = MSE + λ₁·Σ|wᵢ| + λ₂·Σwᵢ²
```

- L1의 희소성(특성 선택) + L2의 안정성을 동시에 얻을 수 있다.
- 상관관계 높은 특성들이 있을 때 L1이 그 중 하나를 임의로 제거하는 문제를 L2가 보완해준다.

---

## 다른 Regularization 기법들과 비교

| 기법 | 아이디어 | 특징 |
|---|---|---|
| **L1 (Lasso)** | 가중치 절대값에 페널티 | 희소 모델, 특성 선택 |
| **L2 (Ridge)** | 가중치 제곱에 페널티 | 부드럽고 안정적 |
| **Elastic Net** | L1 + L2 혼합 | 두 장점 결합 |
| **Dropout** | 학습 중 뉴런 랜덤 비활성화 | 앙상블 효과, 딥러닝에 적합 |
| **Batch Normalization** | 레이어 출력 정규화 | 학습 안정화, 간접 정규화 |
| **Early Stopping** | 검증 손실 상승 시 학습 중단 | 가장 단순, 추가 비용 없음 |
| **Data Augmentation** | 학습 데이터 인위적 증강 | 근본적 해결, 비용 큼 |

**딥러닝에서는** L2보다 Dropout을 더 많이 쓰는 편이고, 둘을 같이 쓰기도 한다. 선형 모델에서는 L1/L2가 기본이다.

---

## 핵심 정리

- 과적합은 **Variance가 높은** 것이며, Regularization은 Bias를 약간 허용하는 대신 Variance를 낮추는 전략이다.
- **L1**은 ±λ 일정 압력 → 작은 가중치를 0으로 제거 → 희소 모델 → 마름모 꼭짓점에서 MSE와 만남.
- **L2**는 2λw 비례 압력 → 모든 가중치를 골고루 축소 → 원 위에서 MSE와 만남, 0이 되지 않음.
- **Elastic Net**은 두 방식의 혼합으로 상관된 특성이 많을 때 유리하다.
- λ는 Cross-Validation으로 튜닝하며, 너무 크면 underfitting이다.
